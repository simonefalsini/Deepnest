================================================================================
SEGMENTATION FAULT - CRASH SEQUENCE DIAGRAM
================================================================================

TIMELINE OF THE CRASH:

FIRST RUN (SUCCESSFUL):
-----------------------

Time 0ms:  startNesting()
           |
           +-> DeepNestSolver::start()
               |
               +-> engine_ = make_unique<NestingEngine#1>()
                   |
                   +-> NestingEngine#1::constructor
                       |
                       +-> Create ParallelProcessor#1
                       +-> Create worker threads (T1, T2, T3)
               |
               +-> engine_->initialize()
               +-> engine_->start()

Time 10ms: step() called in loop
           |
           +-> NestingEngine::step()
               |
               +-> ParallelProcessor#1::processPopulation()
                   |
                   +-> for each individual:
                       |
                       +-> enqueue([&population, &worker, ...])
                           |
                           +-> Task A: queued in io_context
                           +-> Task B: queued in io_context
                           +-> Task C: queued in io_context
                           ...

Time 50ms: Tasks executing on worker threads
           
           Worker T1:          Worker T2:          Main Thread:
           -----------         -----------         ----------
           Executing Task A    Executing Task B    step() enqueuing more tasks
           update fitness      update fitness      ...

Time 100ms: stopNesting() called
            |
            +-> DeepNestSolver::stop()
                |
                +-> engine_->stop()
                    |
                    +-> ParallelProcessor#1::stop()
                        |
                        +-> boost::lock_guard acquired
                        +-> stopped_ = true
                        +-> workGuard_.reset()  ← Allows io_context to exit
                        +-> threads_.join_all() ← Wait for threads
                        |
                        |   [At this point, only 3 tasks executed]
                        |   [4 tasks still in io_context queue!]
                        |
                        +-> ioContext_.stop()   ← PROBLEM: Queue NOT emptied!

At this point:
- Tasks D, E, F, G still in queue
- They capture: [&population#1, &worker#1, ...]
- They WILL execute later when someone calls io_context::poll() or run()


================================================================================
SECOND RUN (CRASH!):
================================================================================

Time 110ms: loadSVG() or similar
            |
            +-> reset() called
                |
                +-> solver_->clear()
                    |
                    +-> PROBLEM: Does NOT destroy engine_!
                        Only clears parts/sheets

Time 120ms: startNesting() called again
            |
            +-> DeepNestSolver::start()
                |
                +-> engine_ = make_unique<NestingEngine>() ← Creates NEW unique_ptr
                    |
                    |   [OLD ENGINE DESTROYED HERE!]
                    |
                    +-> ~NestingEngine#1()
                        |
                        +-> ~ParallelProcessor#1()
                            |
                            +-> Mutex lock acquired
                            +-> stopped_ already true
                            +-> return early
                            |
                            |   [Queue still has Tasks D, E, F, G!]
                            |   [Each captures references to population#1#]
                        |
                        +-> ~Population() ← FREED
                        +-> ~PlacementWorker() ← FREED
                        |
                        |   [But Task lambdas still in OLD io_context!]
                        |   [They still capture [&population#1, &worker#1, ...]]
                    |
                    +-> NestingEngine#2 created (new io_context)
                    +-> ParallelProcessor#2 created (new thread pool)

Time 130ms: step() called on new engine
            |
            +-> NestingEngine#2::step()
                |
                +-> ParallelProcessor#2::processPopulation()
                    |
                    +-> New tasks enqueued to NEW io_context

Time 140ms: [MEANWHILE - RACE CONDITION OCCURS]
            
            Main Thread (NEW):      OLD io_context Queue:
            -----------------      --------------------
            Calling step() on       Task D wakes up and starts
            NestingEngine#2         executing from old queue
                                    |
                                    +-> auto& individuals = 
                                        population.getIndividuals()
                                        ^
                                        This is dangling reference!
                                        (population#1 destroyed)
                                    |
                                    +-> SEGMENTATION FAULT !!!

CRASH SCENARIO VARIATIONS:
--------------------------

Scenario A: Task from old queue executes
            -> Accesses dangling &population reference
            -> Segmentation fault

Scenario B: Task from old queue executes
            -> Accesses dangling &worker reference
            -> worker.placeParts() called on freed object
            -> Segmentation fault

Scenario C: Enqueue race condition
            -> processPopulation() calls enqueue() while stop() is running
            -> enqueue() posts to stopped io_context
            -> Undefined behavior

Scenario D: Memory already reused
            -> New engine allocates same memory address
            -> Old task writes to new object's memory
            -> Data corruption / crash

================================================================================
KEY INSIGHT:
================================================================================

The fundamental issue is that ParallelProcessor's io_context task queue 
has a DIFFERENT LIFETIME than the NestingEngine that created it.

When NestingEngine is destroyed:
  - All member objects are destroyed
  - But queued tasks in io_context still exist
  - Those tasks still capture references to destroyed objects
  - When tasks eventually execute, they access dangling references

This is a classic ASYNCHRONOUS LIFETIME PROBLEM.

The queue is still alive even though the owner is dead!

================================================================================
ROOT CAUSE SEQUENCE:
================================================================================

1. ParallelProcessor::processPopulation() captures [&population, &worker, ...]
2. Tasks enqueued to io_context
3. Some tasks still in queue when stop() called
4. ioContext_.stop() prevents new tasks but doesn't clear queue
5. threads_.join_all() exits, threads stop consuming from queue
6. Old engine destroyed (population, worker freed)
7. New engine created with new objects at different memory addresses
8. Old queued tasks execute with dangling references
9. Access to freed memory = SEGMENTATION FAULT

================================================================================
THE FIX:
================================================================================

To prevent the crash, we must GUARANTEE that:

1. All queued tasks execute BEFORE engine is destroyed
   -> Add ioContext_.poll() loop before stop()

2. Lambda captures can survive engine destruction
   -> Capture by value or use 'this' pointer instead of references

3. Processor is marked stopped before posting
   -> Check stopped_ flag in enqueue()

4. Old processor cleaned up before new one
   -> Call parallelProcessor_.reset() in initialize()

5. Engine destroyed when clearing
   -> Call engine_.reset() in clear()

================================================================================
